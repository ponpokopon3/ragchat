import streamlit as st
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.memory import StreamlitChatMessageHistory, ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import ChatPromptTemplate
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter

# --- ã‚¿ã‚¤ãƒˆãƒ« ---
st.title("ğŸ’¬AIãƒãƒ£ãƒƒãƒˆ + RAGï¼ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢å¼·åŒ–ï¼‰")

# --- APIã‚­ãƒ¼å…¥åŠ›æ¬„ ---
api_key = st.text_input(
    "OpenAI APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆå…¥åŠ›å¾Œã€ã‚¨ãƒ³ã‚¿ãƒ¼ã§ã‚»ãƒƒãƒˆï¼‰",
    type="password",
    value=st.session_state.get("api_key", ""),
)
if api_key:
    st.session_state["api_key"] = api_key
if "api_key" not in st.session_state or not st.session_state["api_key"]:
    st.info("OpenAI APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# --- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ---
uploaded_file = st.file_uploader("æ¤œç´¢å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", type=["txt"])
if uploaded_file:
    raw_text = uploaded_file.read().decode("utf-8")
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    docs = splitter.create_documents([raw_text])
    # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ä½œæˆï¼ˆæ¯å›å†ä½œæˆã®ç°¡æ˜“ä¾‹ã€‚å®Ÿç”¨ã§ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¨å¥¨ï¼‰
    embeddings = OpenAIEmbeddings(openai_api_key=st.session_state["api_key"])
    vectorstore = FAISS.from_documents(docs, embeddings)
else:
    st.info("ã¾ãšæ¤œç´¢å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# --- ãƒãƒ£ãƒƒãƒˆå±¥æ­´ç”¨ãƒ¡ãƒ¢ãƒª ---
msgs = StreamlitChatMessageHistory()
memory = ConversationBufferMemory(
    chat_memory=msgs,
    return_messages=True,
    memory_key="history",
)

# --- ConversationalRetrievalChainï¼ˆRAGãƒã‚§ãƒ¼ãƒ³ï¼‰ ---
llm = ChatOpenAI(
    openai_api_key=st.session_state["api_key"],
    model="gpt-4o",   # æœ€æ–°ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šï¼
    temperature=0.7,
)
prompt = ChatPromptTemplate.from_messages([
    ("system", "ã‚ãªãŸã¯è¦ªåˆ‡ãªAIãƒãƒ£ãƒƒãƒˆã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å†…å®¹ã‚’å‚è€ƒã«ã€å¿…ãšæ—¥æœ¬èªã§ç°¡æ½”ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚"),
    ("human", "{context}\n{history}\n{question}")
])
rag_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=vectorstore.as_retriever(),
    memory=memory,
    combine_docs_chain_kwargs={"prompt": prompt}
)

# --- ãƒãƒ£ãƒƒãƒˆå±¥æ­´è¡¨ç¤º ---
for msg in msgs.messages:
    st.chat_message(msg.type).write(msg.content)

# --- ãƒãƒ£ãƒƒãƒˆå…¥åŠ› ---
user_input = st.chat_input("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›â€¦")
if user_input:
    st.chat_message("user").write(user_input)
    output = rag_chain.run(user_input)
    st.chat_message("assistant").write(output)
