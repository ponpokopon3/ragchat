import streamlit as st
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import ChatPromptTemplate
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter

st.title("ğŸ’¬AIãƒãƒ£ãƒƒãƒˆ + RAGï¼ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢å¼·åŒ–ï¼‰")

# --- APIã‚­ãƒ¼å…¥åŠ›æ¬„ ---
api_key = st.text_input(
    "OpenAI APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆå…¥åŠ›å¾Œã€ã‚¨ãƒ³ã‚¿ãƒ¼ã§ã‚»ãƒƒãƒˆï¼‰",
    type="password",
    value=st.session_state.get("api_key", ""),
)
if api_key:
    st.session_state["api_key"] = api_key
if "api_key" not in st.session_state or not st.session_state["api_key"]:
    st.info("OpenAI APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# --- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ---
uploaded_file = st.file_uploader("æ¤œç´¢å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", type=["txt"])
if uploaded_file:
    if "vectorstore" not in st.session_state or st.session_state.get("uploaded_file_name") != uploaded_file.name:
        raw_text = uploaded_file.read().decode("utf-8")
        splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        docs = splitter.create_documents([raw_text])
        embeddings = OpenAIEmbeddings(openai_api_key=st.session_state["api_key"])
        st.session_state["vectorstore"] = FAISS.from_documents(docs, embeddings)
        st.session_state["uploaded_file_name"] = uploaded_file.name
    vectorstore = st.session_state["vectorstore"]
else:
    st.info("ã¾ãšæ¤œç´¢å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# --- ãƒãƒ£ãƒƒãƒˆå±¥æ­´ç®¡ç†ï¼ˆuser/assistantãƒšã‚¢ã®ã‚¿ãƒ—ãƒ«ã§ä¿æŒï¼‰---
if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = []

# --- RAGãƒã‚§ãƒ¼ãƒ³ ---
llm = ChatOpenAI(
    openai_api_key=st.session_state["api_key"],
    model="gpt-4o",
    temperature=0.7,
)
prompt = ChatPromptTemplate.from_messages([
    ("system", "ã‚ãªãŸã¯è¦ªåˆ‡ãªAIãƒãƒ£ãƒƒãƒˆã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å†…å®¹ã‚’å‚è€ƒã«ã€å¿…ãšæ—¥æœ¬èªã§ç°¡æ½”ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚"),
    ("human", "ã€å‚è€ƒæƒ…å ±ã€‘\n{context}\n\nã€ä¼šè©±å±¥æ­´ã€‘\n{chat_history}\n\nã€è³ªå•ã€‘\n{question}")
])
rag_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=vectorstore.as_retriever(),
    combine_docs_chain_kwargs={"prompt": prompt}
)

# --- ãƒãƒ£ãƒƒãƒˆå±¥æ­´è¡¨ç¤º ---
for role, content in st.session_state["chat_history"]:
    st.chat_message(role).write(content)

# --- ãƒãƒ£ãƒƒãƒˆå…¥åŠ› ---
user_input = st.chat_input("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›â€¦")
if user_input:
    st.chat_message("user").write(user_input)
    # LangChainã®chat_historyå½¢å¼ã¯ [("user", "..."), ("assistant", "..."), ...]
    # assistantç™ºè©±ã‚’ä½œã‚‹å‰ã« userç™ºè©±ã®ã¿ã§æ¨è«–
    output = rag_chain.invoke({
        "question": user_input,
        "chat_history": st.session_state["chat_history"]
    })
    answer = output["answer"] if isinstance(output, dict) and "answer" in output else output
    st.chat_message("assistant").write(answer)
    st.session_state["chat_history"].append(("user", user_input))
    st.session_state["chat_history"].append(("assistant", answer))
